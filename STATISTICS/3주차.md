# 통계학 3주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_3rd_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

2주차는 `2부-데이터 분석 준비하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_3rd_TIL

### 2부. 데이터 분석 준비하기
### 08. 분석 프로젝트 준비 및 기획
### 09. 분석 환경 세팅하기



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | 🍽️      | 
|5주차| 2부 p.203~254  | 🍽️      | 
|6주차| 3부 p.300~356  | 🍽️      | 
|7주차| 3부 p.357~615  | 🍽️      |  

<!-- 여기까진 그대로 둬 주세요-->

# 08. 분석 프로젝트 준비 및 기획

```
✅ 학습 목표 :
* 데이터 분석 프로세스를 설명할 수 있다.
* 비즈니스 문제를 정의할 때 주의할 점을 설명할 수 있다.
* 외부 데이터를 수집하는 방법에 대해 인식한다.
```
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

### 8.1 데이터 분석의 전체 프로세스

_데이터 분석의 목표는 `효과적인 결정`을 할 수 있도록 도움을 주는 것_  

🔢 __데이터 분석의 3단계__
1. `설계 단계`
    - 과제 정의 및 범위 설정
    - 인력 구성 및 PM 확보
    - 실무자 & 데이터 분석가 간 협의 체계 수립

2. `분석 및 모델링 단계`
    - 데이터 분석을 위한 데이터 MART 구축
    - 데이터 준비, 가공, 분석 및 모델 도출
    - 모델 검증 및 실무, 경영진 협의

3. `구축 및 활용 단계`
    - 모델 적용 및 시스템 구축
    - 성과 평가 및 추가, 보완 프로젝트 검토


✏️ __설계 단계__    
    : 데이터 분석에 들어가기에 앞서 `무엇을 하고자 하는지`를 명확히 정의, 프로젝트 수행할 인력 구성    
    ✔️ 실무자와 분석가 간 혐의체계 최적화 필요    
    ✔️ 실시간 피드백 접촉 체계 필요    
    ✔️ 정기적인 미팅 통한 진행 상황 공유 필요

✏️ __분석 및 모델링 단계__    
    : 데이터 분석 및 모델링을 위한 `서버 환경 마련`, 본격적인 `데이터 분석과 모델링`    
    ✔️ 데이터 추출, 검토, 가공, 모델링 등의 세부 절차와 부분 반복 필요    
    ✔️ 모델의 비즈니스적 적합성 심도있게 분석, 성능 평가 중요 (KDD 분석 방법론, CRISP-DM 방법론, SAS사의 SEMMA 방법론)

✏️ __구축 및 활용 단계__    
    : 최종적으로 선정된 모델 `실제 업무의 적용, 성과 측정`         
    ✔️ 분석 모델 적용을 통한 예상 개선 효과를 측정할 때 시스템 적용을 위한 비용 고려 필요     
    ✔️ 모델의 개선효과 측정 및 평가      
    ✔️ 모델 적용 이전/이후 비교 (A/B 테스트)       


🔢 __CRISP-DM 방법론__

1. `비즈니스 이해`
    - 현재 상황 평가
    - 데이터 마이닝 목표 결정
    - 프로젝트 계획 수립
2. `데이터 이해`
    - 데이터 설명
    - 데이터 탐색
    - 데이터 품질 확인
3. `데이터 준비`
    - 데이터 선택
    - 데이터 정제
    - 필수 데이터 구성
    - 데이터 통합
4. `모델링`
    - 모델링 기법 선정
    - 테스트 디자인 생성
    - 모델 생성
    - 모델 평가
5. `평가`
    - 결과 평가
    - 프로세스 검토
    - 다음 단계 결정
6. `배포`
    - 배포 계획
    - 모니터링 및 유지 관리 계획
    - 최종 보고서 작성
    - 프로젝트 검토



🔢 __SAS SEMMA 방법론__
1. `Sampling (데이터 추출)`
    - 전체 데이터에서 분석용 데이터 추출
    - 의미 있는 정보를 추출하기 위한 데이터 분할 및 병합
    - 표본추출을 통해 대표성을 가진 분석용 데이터 생성
    - 분석 모델 생성을 위한 학습, 검증, 테스트 데이터셋 분할
2. `Exploration (데이터 탐색)`
    - 통계치 학인, 그래프 생성 등을 통해 데이터 탐색
    - 상관분석, 클러스터링 등을 통해 변수 간의 관계 파악
    - 분석 모델에 적합한 변수 선정
    - 데이터 현황을 파악하여 비즈니스 아이디어 도출 및 분석 방향 수정
3. `Modification (변수 가공)`
    - 결측값 처리 및 최종 분석 변수 선정
    - 로그변환, 구간화(Binning)등 데이터 가공
    - 주성분분석(PCA)등을 통을 통해 새로운 변수 생성
4. `Modeling (모델 구축)`
    - 다양한 데이터마이닝 기법 적용에 대한 적합성 검토
    - 비즈니스 목적에 맞는 분석 모델 선정, 분석 알고리즘 적용
    - 지도학습, 비지도학습, 강화학습 등 데이터 형태에 따라 알맞은 모델 선정
5. `Assessment (모델 평가)`
    - 구축한 모델들의 예측력 등 성능을 비교, 분석, 평가
    - 비즈니스 상황에 맞는 적정 임계치 (Cut off) 설정
    - 분석 모델 결과를 비즈니스 인사이트에 적용
    - 상황에 따라 추가적인 데이터 분석 수행

![1](../STATISTICS/imgstat/image%20copy%2011.png)


<br>
<br>

---

### 8.2 비즈니스 문제 정의와 분석 목적 도출

_`비즈니스 이해 및 문제 정의`가 조금이라도 잘못되면 최종 인사이트 도출 및 솔루션 적용 단계에서 제대로 된 효과를 보기 힘들다_

🔠 __MECE(Mutually Exclusive Collectively Exhaustive)__   
: 비즈니스 문제를 올바르게 정의하기 위한 논리적 접근법      
    ✔️ 세부 정의들이 서로 겹치지 않고 전체를 합쳤을 때는 빠진 것 없이 완전히 전체를 이루는 것    
    ✔️ 주어진 문제점을 논리적이고 객관적으로 쪼개어 근본적인 원인을 찾아냄    
    ✔️ 일반적으로 로직 트리(Logic Tree)를 활용하여 새부 항목 정리      

🔠 __비즈니스 문제와 목적의 정의__    
: 명확하고 직관적인 한 문장으로 정리할 수 있어야 함

![2](../STATISTICS/imgstat/image%20copy%2012.png)

 💡 비즈니스 문제는 현상에 대한 설명으로 끝나서는 안되고, `본질적인 문제점`이 함께 전달되어야 함!

🔠 __페이오프 매트릭스__ : 문제해결 우선순위 결정방식
![3](../STATISTICS/imgstat/image%20copy%2013.png)

<br>
<br>

---


### 8.3 분석 목적의 전환

_분석 프로젝트의 방향은 언제든 바뀔 수 있다_    
-> 분석 목적의 신속한 전환 필요!   

<br>

---


### 8.4 도메인 지식

🔠 __도매안 지식__    
: 해당되는 분야의 업에 대한 이해도      

 ✔️ 크게 금융, 유통, 제조, 의료 정책 등 업종 단위가 될 수도 있곡 세부적으로는 하나의 기업, 하나의 조직 단위에 대한 이해도가 될 수도 있음   
 ✔️ 해당 분야의 특성과 프로세스를 제대로 파악하지 못한 상태에서는 _문제 정의와 분석 목적이 1차원적일 수밖에 없음_    
✔️ 직접 의미 있는 변수를 찾아내고 분석 방향을 설정하는 것은 도메인 지식이 충분하게 수반됐을 때 가능

✏️ __도메인 지식을 효과적으로 습득하는  법__   
1. 비즈니스 도메인에 소속된 실무자에게 도움받기
2. 관련 논문 참고하기
3. 현장에 방문해 데이터가 만들어지는 과정을 직접 보기

<br>
<br>

---



### 8.5 외부 데이터 수집과 크롤링

_많은 기업들이 데이터 부족으로 인해 외부 데이터를 수집함_

🔢 __외부 데이터 수집 절차__

![4](../STATISTICS/imgstat/image%20copy%2014.png)

✏️ __외부데이터 수집 방법__   
1. 데이터를 판매하는 전문 기업으로부터 구매 or MOU 등을 통해 공유
2. 공공 오픈 데이터 제공 사이트에서 다운로드
3. 데이터 크롤링

❗️ 데이터 크롤링 시 법적인 이슈 고려 필요   
 : 일반적으로 웹사이트에서는 robots.txt 파일을 심어 두어 접속 주체에 따라 `크롤링 허용 범위`를 안내하고 있음
 - User-agent: 대상 크롤러
 - Allow: 허용하는 경로
 - Disallow: 허용하지 않는 경로

 ✔️ 크롤링:웹 페이지가 주어지면 그 페이지 내 링크들을 따라가며 `모든 내용`을 다 가져옴

 ✔️ 스크래핑: 웹 페이지에서 `자신이 원하는 부분`의 정보만 가져옴

 🔠 __크롤링 방법__

 1. Open API를 통해 정리된 데이터 제공받기
 
 2. 직접 웹사이트의 구조를 파악하여, 원하는 데이터를 수집해 오도록 코딩 (파이썬 - `BeautifulSoup`, `Selenium`)

 <br>
 <br>

 


# 09. 분석 환경 세팅하기

```
✅ 학습 목표 :
* 데이터 분석의 전체적인 프로세스를 설명할 수 있다.
* 테이블 조인의 개념과 종류를 이해하고, 각 조인 방식의 차이를 구분하여 설명할 수 있다.
* ERD의 개념과 역할을 이해하고, 기본 구성 요소와 관계 유형을 설명할 수 있다.
```

<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

### 9.1 어떤 데이터 분석 언어를 사용하는 것이 좋을까?

✏️ __SAS__    
✔️ 성능이 확실하고 지원이 잘되기에, 정확성이 중요한 금융업계 기업들이 많이 사용   
✔️ 서비스 지원팀의 존재   
✔️ 기업의 데이터 환경에 맞게 커스텀 받을 수 있음   
✔️ 고급 및 예측 분석 솔루션 제공    
✔️ 데이터 시각화를 쉽게 할 수 있음    
✔️ 딥러닝, 인공신경망 분석에서는 R이나 파이썬보다 약함

✏️ __R__

✔️ 오픈소스임에도 불구하고 ggplot2, ggvis, googleViis 등의 강력한 시각화 패키지 동해 효과적으로 데이터 시각화 가능    
✔️ 활발한 커뮤니티 덕에 문제나 궁금증을 해결할 수 있음    
✔️ 파이썬에 비해 프로그래밍적 소양이 부족해도 사용 가능



✏️ __파이썬__

✔️ 데이터 분석에 국한되지 않고 웹서비스, 응용 프로그램, IoT 등 다양한 분야에서 사용    
✔️ C에 비해 문법이 쉽고 훨씬 간단하지만 C 언어보다 10~350배 정도 느림   
✔️ 10만 개 이상의 패키지 존재 (기계학습 - sklearn, TensorFlow / 데이터 분석 - Pandas, Numpy, matplotlib)    
✔️ 시각화 면에서 R에 비해 구현이 복잡하고 직관적이지 못함

![5](../STATISTICS/imgstat/image%20copy%2015.png)


✏️ __SQL__

✔️ 데이터베이스 시스템에서 `데이터를 관리 및 처리`하기 위해 설계된 언어

✔️ 대화식 언어이기에 명령문이 짧고 간결함

<br>
<br>

---



### 9.2 데이터 처리 프로세스 이해하기

🔢 __데이터 흐름__

: OLTP -> DW(ODS) -> DM -> OLAP

![6](../STATISTICS/imgstat/image%20copy%2016.png)

✏️ __OLTP__ : 실시간으로 데이터를 트랜잭션 단위로 `수집`, `분류`, `저장`하는 시스템

✏️ __DW(Data Warehouse)__ : 수집된 데이터를 사용자 관점에서 주제별로 통합하여 `쉽게 원하는 데이터를 빼낼 수 있도록` 저장해 놓은 통합 데이터베이스 (데이터 창고와 같은 개념)

✏️ __DM(Data Mart)__ : 사용자의 목적에 맞도록 가공된 일부의 `데이터가 저장`되는 곳

✔️ __ETL__ : 데이터의 `추출(Extract)`, `변환(Transorm)`, `불러내기(Load)`를 통해 저장된 데이터를 사용자가 요구하는 포맷으로 변형하여 이동시키는 작업


<br>
<br>

___

### 9.3 분산데이터 처리

🔠 __분산데이터 처리__
1. scale-up: 빅데이터를 처리하기 위해 하나의 컴퓨터의 용량을 늘리고 `더 빠른 프로세서`를 탑재하는 것
2. scale-out: 분선데이터 처리처럼 여러 대의 컴퓨터를 `병렬적으로 연결`하는 것 

_무조건 컴퓨터를 병렬로 연결하면 데이터 처리가 빨라진다_ -> ❌  
_연결된 컴퓨터들이 효율적으로 데이터를 나눠서 처리하고 결과를 취합할 수 있는 기술이 있어야 한다_ -> ⭕️

🔠 __분산처리 방법__

✏️ __HDFS__

✔️ 슬레이브 노드(Slave node): 데이터를 저장하고 계산   
✔️ 마스터 노드(Master node): 대량의 데이터를 HDFS에 저장하고 맵리듀스 방식을 통해 데이터를 병렬 처리    
✔️ 클러이언트 머신 (Client machines): 맵리듀스 작업을 통해 산출된 결과를 사용자에게 보여줌


✏️ __맵리듀스__

✔️ 맵(Map) 단계: 흩어져 있는 데이터를 관련된 데이터끼리 묶어서 임시 집합 생성   
✔️ 리듀스(Reduce) 단계: 필터링과 정렬을 거쳐 데이터 추출   
✔️ 맵리듀스는 key-value 쌍으로 데이터를 처리

✏️ __하둡__

✔️ 하둡 1.0: HOFS, 맵리듀스 탑재   
✔️ 하둡 2.0: 리소스 관리자인 YARN 추가   
✔️ JobTracker: 기본적인 리소스 관리 시스템 (1. 전체 클러스터 리소스 관리  2. 수행중인 잡들의 진행상황, 에러 관리  3. 완료된 잡들의 로그 저장 및 확인)   
✔️ 하둡 2.0의 JobTracker: 리소스 매니저, 애플러케이션 마스터, 타임라인 서버 등으로 분리하여 기능 고도화

🔠 __분산 시스템의 구조__

![7](../STATISTICS/imgstat/image%20copy%2017.png)

- 노드: 하나의 컴퓨터
- 랙: 몇 개의 컴퓨터가 모인 것
- 클러스터: 랙들이 모인 것

🔠 __데이터 분석 환경__

✏️ __스파크__

✔️ 인메모리 기반의 빠른 데이터 처리 가능   
✔️ Java, Scala, 파이썬, R, SQL 등 다양한 언어를 지원함으로써 사용이 편리

-> 데이터 마이닝과 같은 온라인 분석 처리(OLAP) 직압에 주로 사용!

✏️ __제플린__

✔️ 스파크 환경에서의 웹 기반 노트북이며 시각화 툴   
✔️ 순수한 파이썬 언어로도 데이터 가공, 모델링 가능   
✔️ 스파크 전용의 파이썬 코드인 파이스파크, 스파크 환경의 SQL인 Spark SQL 등 사용 가능

<br>
<br>

---


### 9.4 테이블 조인과 정의서 그리고 ERD

🔠 __테이블 조인__
 
✏️ __LEFT JOIN, RIGHT JOIN__

✔️ 하나의 테이블을 기준으로 다른 테이블에서 겹치는 부분 결합   
✔️ 일치하는 키 값이 없는 행은 조인하는 테이블의 값이 `결측값`으로 나타남

✏️ __INNER JOIN__

✔️ 두 테이블 간 `겹치는 부분`의 행만 가져옴

✏️ __FULL JOIN__

✔️ `모든 행`을 살림

✏️ __CROSS JOIN__

✔️ 두 개의 테이블에 있는 `모든 행 조합`   
✔️ 주로 머신러닝에 사용되는 데이터셋 생성 시 사용


🔠 __데이터 단어사전__   
: 각 칼럼과 테이블의 이름을 정할 때 체계를 약속한 일종의 사전
- 일반적으로 축약된 영단어 사용 (ex. CUST(customer), MDCLS_CD(middle class code))

🔠 __메타데이터 관리 시스템__

: 데이터가 어디에 어떻게 저장되어 있는지, 그리고 데이터를 어떻게 사용할 것인지 이해할 수 있도록 데이터에 대한 정보를 관리하는 시스템


🔠 __테이블 정의서__

: 각 DW, DM 등에 적재된 테이블과 칼럼의 한글과 영문명, 데이터 속성, 그리고 간단한 설명 등이 정리된 표

![8](../STATISTICS/imgstat/image%20copy%2018.png)

🔠 __ERD(Entity Relationship Diagram)__

: 각 테이블의 구성 정보와 테이블 간 관계를 도식으로 표현한 그림 형태로 구성

✏️ __물리 ERD__ : 효율적이고 결점 없이 구현하는 것을 목표로 구현하는 ERD

✏️ __논리 ERD__ : 데이터 사용자 입장에서 테이블 간 매핑(Mapping)에 오류가 없으며 데이터의 정규화가 이루어진 ERD -> 데이터 분석가는 이를 보고 DB 구조 파악!

✏️ __기본 키__ : 키 테이블에 적재된 각각의 데이터를 유일하게 구분하는 키 (유일성 & 최소성)

✏️ __외래 키__: 각 테이블 간에 연결을 만들기 위해서 테이블에서 다른 테이블의 참조되는 기본 키

그 외에 `슈퍼 키`(각 행을 유일하게 식별할 수 있는 하나의 키 혹은 조합된 키), `후보 키`(기본키의 조건은 유일성과 최소성을 만족하지만 기본키는 아닌 키) 등이 있음

🔢 __테이블 간의 관계__

 1:1   
 1:N   
 N:N




<br>
<br>

# 확인 문제

## 문제 1.

> **🧚 아래의 테이블을 조인한 결과를 출력하였습니다. 어떤 조인 방식을 사용했는지 맞춰보세요.**

> 사용한 테이블은 다음과 같습니다.

![TABLE1](https://github.com/ejejbb/Template/raw/main/File/2.6.PNG)|![TABLE2](https://github.com/ejejbb/Template/raw/main/File/2.7.PNG)
---|---|

> 보기: INNER, LEFT, RIGHT 조인

<!-- 테이블 조인의 종류를 이해하였는지 확인하기 위한 문제입니다. 각 테이블이 어떤 조인 방식을 이용하였을지 고민해보고 각 테이블 아래에 답을 작성해주세요.-->

### 1-1. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-1.PNG)
```
LEFT JOIN
```

### 1-2. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-3.PNG)
```
INNER JOIN
```

### 1-3. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-2.PNG)
```
RIGHT JOIN
```

### 🎉 수고하셨습니다.